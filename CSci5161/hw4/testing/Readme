These directions for semi-automated testing are similar to those for 
homework 3. The big differences are that we are giving you a working
lexer and parser that you can optionally use if yours is not yet
working and that the main comparisons are now of the expected error
messages, not the abstract syntax that is generated. However, the
testing regime includes a comparison of the abstract syntax, even
though you should be past this testing with homework 3. 

Here are the specific instructions for using the automated testing
framework: 

1. Edit the first two lines of the runsml file to indicate whether
or not you have a working lexer and parser the the next three to
indicate the correct directories for the source code, the test cases
directory and the current directory. (Typically, you will want to
leave the last as is.)

As before, remember that you should not use ~ in paths. You can use
.., though.  

Remember also that the test cases for which we have generated outputs 
are all those in the syntax-and-sem-analysis directory, so it is only
your relevant path to this directory that you should use for the 
automated testing. 

2. Make sure you do not have a file named parse.sml.bak in the code 
directory that you want to preserve; if you do, move it somewhere else
(or change the lines in runsml that make use of this name to save a
copy of the original parse.sml file).

3. Run ./runsml. This will run the driver for all files in the testcases 
directory. Doing so will generate a script of whatever is intended to
be displayed on the monitor/terminal in the file my_script_output and
will compare this to the ones generated by the baseline program that
is in standard_output_script. Additionally, the abstract syntax that
is generated will be compared with that of the baseline program. 

Note that the diff on the output scripts will show differences for every
error in every test file.  This is because the full file name is printed.
As long as your error messages are similar, you should be set. Another
way to use what is provided, in case looking at the diffs repeatedly is too
annoying, is to scan the standard_output_script to see which tiger
programs are the critical ones to examine for errors and to then look
at the outputs for each of these individually (i.e. without this
testing scaffold).

Note also that the comparison of the abstract syntax could be
superfluous at this stage, assuming you have a working parser. (It is
even more superfluous if you don't, because then you will be comparing
the output of my parser with itself.) In this case, you may want to
comment out the relevant parts of the runsml script.

4. If you don't want to run the test for all files, remove the line 
in runsml which creates a listing of the files in the testing directory 
in the file "files.list" and create your own  "files.list" with the 
names of the testcases you need. The diff will show some errors like 
"only present in correct_results".  This is because now we are producing 
the results of only the files in "files.list".

5. If you find it difficult to understand the output of the diff tool on 
the terminal, you could instead use visual diff tools, such as 'meld'. 
Linux machines in both CSE and CS domains have this tool. You can integrate 
this into the runsml script by replacing lines:

	diff standard_output_script my_output_script
	diff correct_outputs outputs

with:
	meld standard_output_script my_output_script &
	meld correct_outputs/ outputs/ &

which will start the GUI interface of meld for comparing the files. If you 
are connecting to these machine remotely using ssh, you will have to use 
the -X flag as shown:

	ssh -X name@machine.cs.umn.edu
