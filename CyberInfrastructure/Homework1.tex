\documentclass[11pt]{article}


\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{color}

\title{Cyberinfrastructure Foundations: Homework 1}
\author{Matthew Le}

\begin{document}
\maketitle



\section{Cyberinfrastructure Vision For $21^{st}$ Century Discover}

\subsection{Chapter 1}

Chapter 1 begins by motivating a need for cyberinfrastucture, providing a few examples of where cyberinfrastucture could be used, such as protein folding, astrophysics applications, and modeling ecological phenomena.  The document proceeds by outlining the NSF's mission as it relates to cyberinfrastructure.  Their mission could be summarized as wanting to promote collaboration, education, and research in the field of science and engineering.  Chapter 1 concludes by outlining some goals that the NSF has with respect to cyberinfrastructure, and how they plan on achieving those goals.

\subsection{Chapter 2}

Chapter 2 starts out by motivating the need for high performance computing (HPC), stating that it can be effectively used for things such as analyzing the human genome, modeling the behavior of large societies, and simulating the earth's climate.  They further state that HPC tools help improve our nation's competitiveness in terms of both education and research.  The second section of this chapter goes into details about the NSF's five year plan in terms of HPC, stating that they hope to enable petascale computing for use in research and education.  The next few paragraphs go into details about how they plan to go about achieving this goal.  One such suggestion was to implement a resource-sharing plan with other federal agencies, since the hardware and equipment used in HPC can be quite expensive.  Diving the cost amongst many agencies will help to alleviate this burden.  The last section addresses the problem of utilizing these resources, stating that a large amount of software will need to be reengineered to utilize these new architectures.  

\subsection{Chapter 3}
Chapter 3 discusses data and how it relates to cyberinfrastructure.  They state that data is becoming widely available in a lot of settings due to advances in digital technologies, instrumentation, and pervasive networks.  They go on to talk about strategies for collecting data.  One of the major issues that comes up is that data is shared and owned by a wide range of communities from all around the world, so they must work together with many other organizations in order to do this effectively.  

\newpage
\section{Revolutionizing Science and Engineering...}
The trends and issues are categorized into three sections: computation, content, and interaction
\subsection{Computation}
Hardware components are increasing at exponential rates, however, circuit speeds are not increasing, but circuit densities are.  Disk storage has also been increasing at a very high rate, especially over the last few years.  They state that disk capacity has increased 60 \% per year on average, however, in the last few years it has been increasing by almost 100 \% every year.  A third shift in computing resources comes from networks.  Researchers have been able to achieve network bandwidths as high as 2.5 gigabits/s, with 45 megabits/s networks being easily attainable in the general public.  Researchers have also made strides in building optical networks, which are able to achieve very high bandwidths.

With these trends come complications.  One major complication with emerging parallel and distributed architectures is being able to effectively program them.  Parallel programming is often difficult and can be difficult to use, especially for researchers outside the area of computer science.  Another issue that arises with evolving resources is the task of sustaining access to the most recent and cutting edge hardware.  Since these resources evolve so quickly, they can become obsolete very soon as well.  Continuing to keep up with these trends can be quite expensive.  

\subsection{Content}
Data and information is also evolving very rapidly and becoming much more readily available.  Important collections of data regarding social, biological, and physical sciences are accessible via the internet, and can be utilized by researchers around the world.  The article states that high energy physics researchers have compiled nearly an exabyte of data from four major large hadron collider experiments.  The major issue that accompanies data collection is how to collect it effectively, organize it in a way that makes it useful and efficiently accessible, and how to analyze it.

\subsection{Interaction}
In terms of interaction, cyberinfrastructure has made large strides in helping researchers collaborate with one another.  For example, many communities have developed comprehensive collections of online literature resources, such as the ACM and other similar organizations.  Additionally, there are a number of online data repositories that are shared between many researchers, that aid in collaboration.  

\subsection{Cyberinfrastructure's effects on Science and Engineering}
Cyberinfrastructure will change the way that science and engineering researchers collect, organize, and analyze their data.  The document gives a number of how specific fields will be changed.  For example, with respect to atmospheric science, the ACP is planning to enable the development of earth system models that are capable of simulating the earth's climate for thousands of years.  

\newpage
\section{Understanding Infrastructure ...}
Below is a list of recommendations made in the document

\subsection{Learning from Cyberinfrastructure}
\begin{itemize}
\item Cyberinfrastructure projects should be studied at a high level with regard to their handling of social, cultural, and organizational issues
\item Project reporting should encourage honest responses, rather than simply emphasizing the successes.
\item Provide incentives for contributing to cyberinfrastructure activities, such as creating and sharing data effectively
\item Standard protocols should be built into the terms of grants for data collection and management of funded projects
\end{itemize}

\subsection{Improving cyberinfrastructural practice}
\begin{itemize}
\item Improve training for information managers
\item Create interdisciplinary meetings for young researchers such as grad students, post docs, and new faculty to collaborate
\item Experiment with extending the lifetimes of certain grants.
\end{itemize}

\subsection{Enhancing resiliency, sustainability, and reach}

\begin{itemize}
\item We should support a diverse set of cyberinfrastructure research
\item Encourage under-represented groups and minorities to take part in cyberinfrastructure research and education
\item Partner together with experts in other domains that can help further cyberinfrastructure research.
\end{itemize}

\newpage

\section{Cyberinfrastructure Project}
One project that I personally have experience with is the Minnesota Expeditions in Computing project, where they are developing data driven approaches for furthering our understanding of climate change.

\subsection{Background}
Climate change is one of the most important issues that our generation has been faced with.  That said, the fast majority of research done in this field is done using physics-based models.  This group is concerned with using data driven approaches for furthering our understanding of climate change.  It consists of researchers from a number of universities, such as University of Minnesota, North Carolina A \& T, NC State, Northeastern, and Northwestern University.

\subsection{Motivation}
Now that data is becoming ubiquitous, there is growing interest in analyzing climate data to inform us about our environment, rather than relying on physics-based models.  In addition to this, great strides have been made in the area of high performance computing, which allows them to more efficiently analyze the data and/or be more comprehensive about their studies.

\subsection{Goals}
The main goal of this project is to provide insights about our environment by analyzing the vast amounts of data available throughout the climate science community.   In addition to this, they hope to make contributions to the high performance and parallel computing communities as well.  

\subsection{Accomplishments}
This group has been awarded the NSF Expeditions in Computing grant, which is a 5-year, \$10 million project, and is perhaps the most prestigious grant attainable in the area of computer science.  In addition to this, they have published 22 papers in 2013 alone, in prestigious top tier conferences such as: IJCAI, HPDC, ICDM, AAAI, and KDD.  


\subsection{Cyberinfrastructure}
The Minnesota Expeditions group uses a supercomputer provided by the Minnesota Supercomputing Institute (MSI) from within the University of Minnesota.  This provides the group with multi-core computing resources, GPU resources, and a very large amount of disk space, something that is absolutely essential for the large data sets they work on.  

\subsection{Future Plans}
In the future, this group plans on continuing their research in climate data mining, and publishing their results for others to learn from.

\newpage

\section{Big Table}
Big Table is a scalable distributed storage system used by Google.  The goal is to have it scale to magnate petabytes of data across thousands of servers.  Big Table is used by a wide spectrum of applications within google, that have very different demands, placing different restrictions on the design, such as data size and latency requirements. 

In order to meet these requirements, Google has built BigTable on top of the Google File System, which is a distributed file system.  The table is broken up among rows, each of which can potentially be managed by a different server.  The table is represented as a sparse table, which is able to save space when there are missing entries in the table.  The keys in this table are then hashed and stored in a sorted order, allowing for quick lookups.  

\section{Replication}
Replication is used to improve reliability and fault tolerance within a distributed system.  The idea is to make copies of data and store them on different machines in different locations.  This way if one of the machines goes down, the data is not lost as it is backed up on a different machine in a different location.  In addition to improving fault tolerance, this can also have impacts on performance.  If a user is trying to access a piece of data and it exists in multiple places, they can get it from the nearest server, which might have a shorter transfer time.

One potential issue with replication is synchronization.  If a user updates his/her data on one machine, then all replications also need to be updated.  This carries many of the same issues of synchronization that come with parallel an concurrent computing.  A second issue is what to do if all the data doesn't fit in one machine.  




\end{document}






