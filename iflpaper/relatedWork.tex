%!TEX root = paper.tex
\section{Related Work}
This work builds on two broad categories of related projects, those that deal with deterministic parallelism in the presence of shared state, and those that deal with speculative parallelism.

\subsection{Shared State}
IVars were first proposed in the language Id \cite{i-structures}, which is also a parallel functional language, however, they sacrifice determinism by also adding MVars, which are shared references that can be written an arbitrary number of times with implicit synchronization.  More recently, IVars have been adopted by parallel languages such as the Par Monad of Haskell \cite{par-monad} and some of the Concurrent Collections work\cite{cnc}, however, neither of these works support speculative parallelism.  

LVars are a new abstraction that were recently proposed by Kuper et al. \cite{lvars-fhpc, freeze-after-writing} that generalize IVars to allow multiple writes, but restrict that they must be monotonically increasing in some fashion.   LVars suffer from the same problem as IVars in that they also lose their determinism guarantee in the presence of cancelation.  More recently, they have proposed an elegant solution for combining LVars with speculative parallelism \cite{lvars-pldi}, where threads can perform speculative work (i.e. can potentially be canceled) if they are read only.  They do however, allow speculative threads to write to memoization tables such that they can ``help out'' other threads, however, one shortcoming to this solution is that performance becomes difficult to reason about as a programmer.  Note that parallel speedup is only achieved if the speculative thread is able to write to the memoization table before another thread needs this result.  If it does not make it there in time, then not only is there no benefit, but the speculative thread corresponds to wasted work.  On the other hand in this work, if the commit  portion of a parallel tuple finishes before the speculative threads, it simply waits for them to complete and then joins with them.

Welc et al. proposed a solution for enforcing a sequential semantics for Java futures \cite{safe-futures, quasi-safe-futures}, a concurrency abstraction taken from Multilisp \cite{multilisp}.  They too extend their runtime system to enforce deterministic execution, but in a very different way relative to our approach.  First, for each thread that is spawned, they create a new copy for each object that it writes to.  This does not allow for the type of fine grained sharing that we are able to support in our producer-consumer benchmark.  Additionally, if their runtime system detects that a thread has violated the sequential semantics, they restart the thread from the beginning, where as our approach is able to simply rollback a thread to the exact point in which the violation occurred, avoiding redundant work.  

Bocchino et al. give a region based type and effect system for guaranteeing determinism at compile time for parallel Java programs \cite{type-and-effect-dpj}.  Their approach requires annotations on Java programs specifying what ``regions'' objects are allocated in.  They then extend their Java compiler to statically verify that concurrently executing threads do not manipulate objects that are allocated in the same regions.  

\subsection{Speculative Parallelism}

There is a large body of work that has been done on transparent speculative parallelism, where the compiler and runtime system automatically perform value prediction and control the amount of parallelism in the program, however, more relevant to this work is the notion of programmable speculative parallelism.  Programmable speculative parallelism was first introduced in \cite{burton-spec-par} in the context of the Mirranda language.  Their approach uses a purely functional language, so they do not deal with any of the rollback issues that we present in this work. 

More recently, Prabhu et al. propose language constructs for specifying speculatively parallel algorithms and formalize their semantics using the lambda calculus extended with shared references \cite{programmable-spec-par}.  Rather than providing a runtime system that performs rollbacks in the event of a miss-speculated value, they describe an analysis that is performed at compile-time that guarantees that they will never need to perform any rollbacks.  Their analysis is necessarily conservative, making certain types of sharing patterns not expressible in their language. 

Software Transactional Memory (STM) can be seen as a form of speculative parallelism.  Transactional memory allows programmers to wrap code in ``atomic'' blocks that the runtime system guarantees to be executed in isolation \cite{stm}.  STM uses a form of ``optimistic'' concurrency where threads execute code inside of transactions and upon completion check to see if any of the memory locations they read or wrote were compromised by other concurrently running threads.  If so, they abort the transaction and restart from the beginning.  Transactional memory is different from our work in the sense that they provide no guarantees about deterministic execution, and is concerned only with atomicity. 




